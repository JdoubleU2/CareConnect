{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f75cd3-1fd9-4b7f-bbca-71aed0df175c",
   "metadata": {
    "language": "python",
    "name": "deps"
   },
   "outputs": [],
   "source": [
    "# Install Pytorch & other libraries\n",
    "%pip install \"torch>=2.4.0\" tensorboard\n",
    "\n",
    "# Install Gemma release branch from Hugging Face\n",
    "%pip install git+https://github.com/huggingface/transformers@main\n",
    "\n",
    "# Install Hugging Face libraries\n",
    "%pip install  --upgrade \\\n",
    "  \"datasets==3.3.2\" \\\n",
    "  \"accelerate==1.4.0\" \\\n",
    "  \"evaluate==0.4.3\" \\\n",
    "  \"bitsandbytes==0.45.3\" \\\n",
    "  \"trl==0.15.2\" \\\n",
    "  \"peft==0.14.0\" \\\n",
    "  \"protobuf\" \\\n",
    "  \"sentencepiece\"\n",
    "\n",
    "!python -c \"from accelerate.utils import write_basic_config; write_basic_config(mixed_precision='fp16')\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e0ef14-3e64-44b1-8ee1-fde7d39e6089",
   "metadata": {
    "language": "python",
    "name": "cell8"
   },
   "outputs": [],
   "source": [
    "!accelerate env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae71215-a550-4c02-ae94-3ff4aa233d9b",
   "metadata": {
    "language": "python",
    "name": "imports"
   },
   "outputs": [],
   "source": [
    "\n",
    "import datetime\n",
    "import json\n",
    "import torch\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import Dataset, load_dataset\n",
    "from accelerate import Accelerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62e39d5-4f5c-4f58-8540-80a61496d424",
   "metadata": {
    "language": "python",
    "name": "huggingface_login"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Set your API token here\n",
    "HUGGINGFACE_TOKEN = \"\"\n",
    "\n",
    "# Log in with the token\n",
    "def login_to_huggingface_token(token):\n",
    "    try:\n",
    "        login(token=token)\n",
    "        print(\"✅ Successfully logged into Hugging Face!\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Login failed: {e}\")\n",
    "\n",
    "# Login using the token\n",
    "login_to_huggingface_token(HUGGINGFACE_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3425cc41-504b-400e-8951-50b63fec7b8b",
   "metadata": {
    "language": "python",
    "name": "check_gpu"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Current GPU memory: {torch.cuda.mem_get_info()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4505013f-955f-4180-babf-4bac57d1ca11",
   "metadata": {
    "language": "python",
    "name": "get_training_data"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import tempfile\n",
    "import glob\n",
    "from datasets import Dataset\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "\n",
    "def convert_to_conversation(sample):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": sample['instruction']},\n",
    "            {\"role\": \"user\", \"content\": sample['input']},\n",
    "            {\"role\": \"assistant\", \"content\": sample['output']}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def load_data_from_snowflake_stage():\n",
    "    # Create dataset list\n",
    "    dataset = []\n",
    "    \n",
    "    # Get the active Snowpark session\n",
    "    session = get_active_session()\n",
    "    print(f\"DEBUG: Got active Snowpark session\")\n",
    "    \n",
    "    # Define schema and stage\n",
    "    schema = \"GIT\"\n",
    "    stage = \"CARECONNECT_TRAINING_DATA_STAGE\"\n",
    "    fully_qualified_stage = f\"{schema}.{stage}\"\n",
    "    print(f\"DEBUG: Using fully qualified stage: {fully_qualified_stage}\")\n",
    "    \n",
    "    # Set the schema for the session\n",
    "    session.use_schema(schema)\n",
    "    print(f\"DEBUG: Set active schema to {schema}\")\n",
    "    \n",
    "    # List files in the stage\n",
    "    files_in_stage = session.sql(f\"LIST @{fully_qualified_stage}\").collect()\n",
    "    print(f\"DEBUG: Found {len(files_in_stage)} files in stage\")\n",
    "    for i, file_info in enumerate(files_in_stage[:5]):  # Print first 5 files\n",
    "        print(f\"DEBUG: File {i+1}: {file_info}\")\n",
    "    \n",
    "    # Create a temporary directory to store downloaded files\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        print(f\"DEBUG: Created temp directory: {temp_dir}\")\n",
    "        \n",
    "        jsonl_files_count = 0\n",
    "        processed_files_count = 0\n",
    "        \n",
    "        for file_info in files_in_stage:\n",
    "            file_path = file_info[\"name\"]\n",
    "            file_name = os.path.basename(file_path)\n",
    "            \n",
    "            # Only process .jsonl files\n",
    "            if not file_name.endswith(\".jsonl\"):\n",
    "                continue\n",
    "                \n",
    "            jsonl_files_count += 1\n",
    "            \n",
    "            # Create a subdirectory for each file to avoid conflicts\n",
    "            file_dir = os.path.join(temp_dir, f\"file_{jsonl_files_count}\")\n",
    "            os.makedirs(file_dir, exist_ok=True)\n",
    "            \n",
    "            # Download file from stage to local temp directory\n",
    "            print(f\"DEBUG: Downloading {file_name} to {file_dir}\")\n",
    "            try:\n",
    "                session.file.get(f\"@{fully_qualified_stage}/{file_name}\", file_dir)\n",
    "                print(f\"DEBUG: Download command executed for {file_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR: Failed to download {file_name}: {str(e)}\")\n",
    "                continue\n",
    "            \n",
    "            # Find the downloaded file(s)\n",
    "            downloaded_files = glob.glob(os.path.join(file_dir, \"**\", \"*\"), recursive=True)\n",
    "            print(f\"DEBUG: Found {len(downloaded_files)} files in download directory\")\n",
    "            for df in downloaded_files[:5]:  # Print first 5 files\n",
    "                print(f\"DEBUG: Downloaded file: {df}\")\n",
    "            \n",
    "            # Process each downloaded file\n",
    "            jsonl_files = [f for f in downloaded_files if f.endswith(\".jsonl\") and os.path.isfile(f)]\n",
    "            if not jsonl_files:\n",
    "                print(f\"WARNING: No .jsonl files found in download directory for {file_name}\")\n",
    "                continue\n",
    "                \n",
    "            for jsonl_file in jsonl_files:\n",
    "                print(f\"DEBUG: Processing {jsonl_file}\")\n",
    "                if os.path.exists(jsonl_file):\n",
    "                    file_size = os.path.getsize(jsonl_file)\n",
    "                    print(f\"DEBUG: File size: {file_size} bytes\")\n",
    "                else:\n",
    "                    print(f\"ERROR: File doesn't exist: {jsonl_file}\")\n",
    "                    continue\n",
    "                \n",
    "                # Parse the downloaded JSONL file\n",
    "                records_count = 0\n",
    "                try:\n",
    "                    with open(jsonl_file, \"r\", encoding=\"utf-8\") as file:\n",
    "                        for line_num, line in enumerate(file, 1):\n",
    "                            try:\n",
    "                                record = json.loads(line.strip())\n",
    "                                dataset.append(record)\n",
    "                                records_count += 1\n",
    "                                \n",
    "                                # Print sample of first record in each file\n",
    "                                if line_num == 1:\n",
    "                                    print(f\"DEBUG: Sample record from {os.path.basename(jsonl_file)}:\")\n",
    "                                    # Print first few keys/values for sample\n",
    "                                    sample_data = {k: str(v)[:50] + \"...\" if isinstance(v, str) and len(str(v)) > 50 else v \n",
    "                                                  for k, v in list(record.items())[:3]}\n",
    "                                    print(f\"DEBUG: {sample_data}\")\n",
    "                                    \n",
    "                            except json.JSONDecodeError as e:\n",
    "                                print(f\"ERROR: Failed to parse JSON at line {line_num} in {jsonl_file}: {e}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"ERROR: Failed to read {jsonl_file}: {str(e)}\")\n",
    "                    continue\n",
    "                \n",
    "                print(f\"DEBUG: Processed {records_count} records from {os.path.basename(jsonl_file)}\")\n",
    "                processed_files_count += 1\n",
    "        \n",
    "        print(f\"DEBUG: Processed {processed_files_count} out of {jsonl_files_count} JSONL files\")\n",
    "        print(f\"DEBUG: Total records in dataset: {len(dataset)}\")\n",
    "    \n",
    "    # Convert dataset to conversation format\n",
    "    print(\"DEBUG: Converting to conversation format...\")\n",
    "    if not dataset:\n",
    "        print(\"ERROR: No data was loaded into the dataset!\")\n",
    "        return Dataset.from_dict({\"messages\": []})\n",
    "        \n",
    "    data_temp = [convert_to_conversation(data) for data in dataset]\n",
    "    \n",
    "    print(f\"DEBUG: Created {len(data_temp)} conversation entries\")\n",
    "    \n",
    "    # Print sample of conversation format\n",
    "    if data_temp:\n",
    "        print(\"DEBUG: Sample conversation format:\")\n",
    "        print(data_temp[0])\n",
    "    \n",
    "    data_dict = {\n",
    "        \"messages\": [item[\"messages\"] for item in data_temp]\n",
    "    }\n",
    "    \n",
    "    print(f\"DEBUG: Final dataset structure has {len(data_dict['messages'])} messages\")\n",
    "    \n",
    "    # Create dataset from dictionary\n",
    "    huggingface_dataset = Dataset.from_dict(data_dict)\n",
    "    print(f\"DEBUG: Created Hugging Face dataset with shape: {huggingface_dataset.shape}\")\n",
    "    \n",
    "    return huggingface_dataset\n",
    "\n",
    "# Load the dataset from Snowflake stage\n",
    "print(\"Starting to load data from Snowflake stage...\")\n",
    "try:\n",
    "    dataset = load_data_from_snowflake_stage()\n",
    "    print(f\"Dataset loaded successfully with {len(dataset)} entries\")\n",
    "\n",
    "    # Print dataset info\n",
    "    print(\"\\nDATASET INFO:\")\n",
    "    print(dataset)\n",
    "    dataset = dataset.shuffle()\n",
    "    print(\"\\nDATASET FEATURES:\")\n",
    "    print(dataset.features)\n",
    "    \n",
    "    # Show a sample from the dataset\n",
    "    if len(dataset) > 0:\n",
    "        print(\"\\nSAMPLE FROM DATASET:\")\n",
    "        print(dataset[0])\n",
    "\n",
    "   \n",
    "    split_dataset = dataset.train_test_split(test_size=0.2)  # 20% test, 80% train\n",
    "    \n",
    "    # Now you can access the train and test splits:\n",
    "    train_dataset = split_dataset[\"train\"]\n",
    "    test_dataset = split_dataset[\"test\"]\n",
    "    \n",
    "    print(\"Train dataset size:\", len(train_dataset))\n",
    "    print(\"Test dataset size:\", len(test_dataset))\n",
    "    \n",
    "    # Before training, you might need to flatten the Messages\n",
    "    def flatten_messages(example):\n",
    "        return {\n",
    "        \"text\": tokenizer.apply_chat_template(\n",
    "            example[\"Messages\"], \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "    }\n",
    "\n",
    "    # Apply the flattening\n",
    "    dataset = dataset.map(flatten_messages, remove_columns=[\"Messages\"])\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to load dataset: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faf4be5-c6ae-4867-b9b8-a3392ca347d5",
   "metadata": {
    "language": "python",
    "name": "ALL_VARIABLES_CONFIGUREATION"
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"google/gemma-3-4b-pt\"\n",
    "FINETUNED_MODEL_DIR = \"./Careconnect-gemma3-huggingface\"\n",
    "NUM_GPUS = torch.cuda.device_count()\n",
    "MAX_SEQ_LENGTH = 512\n",
    "BATCH_SIZE_PER_GPU = 2  #\n",
    "GRADIENT_ACCUMULATION_STEPS = 4  # Adjust based on effective batch size needed\n",
    "NUM_EPOCHS = 0.05  # Increase later from 0.05\n",
    "LEARNING_RATE = 2e-4  # Slightly increased\n",
    "LORA_RANK = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "SAVE_STEPS = 500\n",
    "LOGGING_STEPS = 10\n",
    "EVAL_STEPS = 500\n",
    "WARMUP_STEPS = 100\n",
    "FINETUNED_WEIGHTS_PATH = f\"{FINETUNED_MODEL_DIR}/model.weights.h5\"\n",
    "FINETUNED_VOCAB_PATH = f\"{FINETUNED_MODEL_DIR}/vocabulary.spm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749b312d-0541-4e89-b24c-14d9eb1c8413",
   "metadata": {
    "language": "python",
    "name": "nvidia_smi"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504dc8b6-fa52-43cd-8b50-cc39b9d9f15a",
   "metadata": {
    "language": "python",
    "name": "load_tokenizer"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the Gemma tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-1b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ff0dce-ef2e-43ed-8bc9-e79c9782377f",
   "metadata": {
    "language": "python",
    "name": "load_model"
   },
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# Get the current CUDA device\n",
    "current_device = torch.cuda.current_device()\n",
    "\n",
    "# Select model class based on id\n",
    "model_id = MODEL_NAME\n",
    "model_class = AutoModelForCausalLM\n",
    "\n",
    "# Check if GPU benefits from bfloat16\n",
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    torch_dtype = torch.bfloat16\n",
    "else:\n",
    "    torch_dtype = torch.float16\n",
    "\n",
    "# Define model init arguments\n",
    "model_kwargs = dict(\n",
    "    attn_implementation=\"eager\", # Use \"flash_attention_2\" when running on Ampere or newer GPU\n",
    "    torch_dtype=torch_dtype, # What torch dtype to use, defaults to auto\n",
    "    device_map=\"auto\", # Let torch decide how to load the model\n",
    ")\n",
    "\n",
    "# BitsAndBytesConfig: Enables 4-bit quantization to reduce model size/memory usage\n",
    "model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=model_kwargs['torch_dtype'],\n",
    "    bnb_4bit_quant_storage=model_kwargs['torch_dtype'],\n",
    ")\n",
    "\n",
    "model = model_class.from_pretrained(model_id, **model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10b0d67-a69e-4328-b15c-a5064bd3f59e",
   "metadata": {
    "language": "python",
    "name": "PeFT"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    r=LORA_RANK,\n",
    "    bias=\"none\",\n",
    "    target_modules=\"all-linear\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    modules_to_save=[\"lm_head\", \"embed_tokens\"] # make sure to save the lm_head and embed_tokens as you train the special tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a916ac97-4ccd-4862-96bc-acc7f71b2c81",
   "metadata": {
    "language": "python",
    "name": "test_examples"
   },
   "outputs": [],
   "source": [
    "TEST_EXAMPLES = [\n",
    "    \"Who are you? What is your name?\"\n",
    "    \"What are the treatments for Diabetic Neuropathies: The Nerve Damage of Diabetes?\",\n",
    "    \"My BMI is 22.81, and I do not smoke. I do not drink regularly. I do not have diabetes. I have not had a stroke. I do engage in physical activity. My general health is Very good. I sleep 8.0 hours per night. I do not have asthma. I do not have skin cancer. But i am very ill, what could be wrong with me?\",\n",
    "    \"I have Variable, including almost any neurological symptom or sign, with autonomic, visual, motor, and sensory problems being the most common. What do you think I might have?\",\n",
    "    \"What are the risks of taking Brozeet-LS 1mg Syrup?\",\n",
    "    'What should I do if I have a sore throat?',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b25bd32-5ad7-4700-bc8c-83fe73fc4378",
   "metadata": {
    "language": "python",
    "name": "def_trainer"
   },
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "args = SFTConfig(\n",
    "    output_dir= FINETUNED_MODEL_DIR,         # directory to save and repository id\n",
    "    max_seq_length=MAX_SEQ_LENGTH,                     # max sequence length for model and packing of the dataset\n",
    "    packing=True,                           # Groups multiple samples in the dataset into a single sequence\n",
    "    num_train_epochs=NUM_EPOCHS,                     # number of training epochs\n",
    "    per_device_train_batch_size=BATCH_SIZE_PER_GPU,          # batch size per device during training\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,          # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n",
    "    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n",
    "    logging_steps=LOGGING_STEPS,                       # log every 10 steps\n",
    "    save_strategy=\"epoch\",                  # save checkpoint every epoch\n",
    "    learning_rate=2e-4,                     # learning rate, based on QLoRA paper\n",
    "    fp16=True if torch_dtype == torch.float16 else False,   # use float16 precision\n",
    "    bf16=True if torch_dtype == torch.bfloat16 else False,   # use bfloat16 precision\n",
    "    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n",
    "    push_to_hub=True,                       # push model to hub\n",
    "    report_to=\"tensorboard\",                # report metrics to tensorboard\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False, # We template with special tokens\n",
    "        \"append_concat_token\": True, # Add EOS token as separator token between examples\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create Trainer object\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=tokenizer\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c899a4c-a97d-4e6d-b314-96b48ac227c3",
   "metadata": {
    "language": "python",
    "name": "finetune"
   },
   "outputs": [],
   "source": [
    "# Fine-tune the model\n",
    "print(\"Starting fine-tuning...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5b0e9e-5a1d-4a44-a88e-b6b36e188900",
   "metadata": {
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7219f3-ba3d-4f65-8b99-598e4c9be28e",
   "metadata": {
    "language": "python",
    "name": "save_model"
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# Load Model base model\n",
    "model = model_class.from_pretrained(MODEL_NAME, low_cpu_mem_usage=True)\n",
    "\n",
    "# Merge LoRA and base model and save\n",
    "peft_model = PeftModel.from_pretrained(model, FINETUNED_MODEL_DIR)\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"CareConnect-gemma3-4b-pt\", safe_serialization=True, max_shard_size=\"2GB\")\n",
    "\n",
    "processor = AutoTokenizer.from_pretrained(FINETUNED_MODEL_DIR)\n",
    "processor.save_pretrained(\"CareConnect-gemma3-4b-pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8c82dc-56df-4cba-882c-35efa669b3fe",
   "metadata": {
    "language": "python",
    "name": "free_memory"
   },
   "outputs": [],
   "source": [
    "# free the memory again\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e3b185-2854-4869-93db-8e732895bbf4",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "model_id = \"CareConnect-gemma3-4b-pt\"\n",
    "\n",
    "# Load Model with PEFT adapter\n",
    "model = model_class.from_pretrained(\n",
    "  model_id,\n",
    "  device_map=\"auto\",\n",
    "  torch_dtype=torch_dtype,\n",
    "  attn_implementation=\"eager\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7b7fc0-8d65-49ce-85ca-f9739cb17fe5",
   "metadata": {
    "language": "python",
    "name": "cell2"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1e24c9-78f3-403d-916f-3cc2b8d0f794",
   "metadata": {
    "language": "python",
    "name": "cell3"
   },
   "outputs": [],
   "source": [
    "!ls Careconnect-gemma3-huggingface "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6b67f0-3b34-48c6-8a5d-42745dfd9c7d",
   "metadata": {
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": [
    "!ls CareConnect-gemma3-4b-pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b5ecb3-2166-4e16-9d06-cdc7306b68ef",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02947d2-5b94-4045-9dc0-cbb0ea2c5f89",
   "metadata": {
    "language": "python",
    "name": "cell6"
   },
   "outputs": [],
   "source": [
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()\n",
    "\n",
    "put_result = session.file.put(\"/home/app/CareConnect-gemma3-4b-pt/added_tokens.json\",\"@SOFTWARESURGEONS_DB.GIT.CARECONNECT_GEMMA3_STAGE\", auto_compress= False, overwrite= True)\n",
    "put_result = session.file.put(\"/home/app/CareConnect-gemma3-4b-pt/config.json\",\"@SOFTWARESURGEONS_DB.GIT.CARECONNECT_GEMMA3_STAGE\", auto_compress= False, overwrite= True)\n",
    "put_result = session.file.put(\"/home/app/CareConnect-gemma3-4b-pt/generation_config.json\",\"@SOFTWARESURGEONS_DB.GIT.CARECONNECT_GEMMA3_STAGE\", auto_compress= False, overwrite= True)\n",
    "put_result = session.file.put(\"/home/app/CareConnect-gemma3-4b-pt/model-00001-of-00010.safetensors\",\"@SOFTWARESURGEONS_DB.GIT.CARECONNECT_GEMMA3_STAGE\", auto_compress= False, overwrite= True)\n",
    "put_result = session.file.put(\"/home/app/CareConnect-gemma3-4b-pt/model-00002-of-00010.safetensors\",\"@SOFTWARESURGEONS_DB.GIT.CARECONNECT_GEMMA3_STAGE\", auto_compress= False, overwrite= True)\n",
    "put_result = session.file.put(\"/home/app/CareConnect-gemma3-4b-pt/model-00003-of-00010.safetensors\",\"@SOFTWARESURGEONS_DB.GIT.CARECONNECT_GEMMA3_STAGE\", auto_compress= False, overwrite= True)\n",
    "put_result = session.file.put(\"/home/app/CareConnect-gemma3-4b-pt/model-00004-of-00010.safetensors\",\"@SOFTWARESURGEONS_DB.GIT.CARECONNECT_GEMMA3_STAGE\", auto_compress= False, overwrite= True)\n",
    "put_result = session.file.put(\"/home/app/CareConnect-gemma3-4b-pt/model-00005-of-00010.safetensors\",\"@SOFTWARESURGEONS_DB.GIT.CARECONNECT_GEMMA3_STAGE\", auto_compress= False, overwrite= True)\n",
    "put_result = session.file.put(\"/home/app/CareConnect-gemma3-4b-pt/model-00006-of-00010.safetensors\",\"@SOFTWARESURGEONS_DB.GIT.CARECONNECT_GEMMA3_STAGE\", auto_compress= False, overwrite= True)\n",
    "put_result = session.file.put(\"/home/app/CareConnect-gemma3-4b-pt/model-00007-of-00010.safetensors\",\"@SOFTWARESURGEONS_DB.GIT.CARECONNECT_GEMMA3_STAGE\", auto_compress= False, overwrite= True)\n",
    "put_result = session.file.put(\"/home/app/CareConnect-gemma3-4b-pt/model-00008-of-00010.safetensors\",\"@SOFTWARESURGEONS_DB.GIT.CARECONNECT_GEMMA3_STAGE\", auto_compress= False, overwrite= True)\n",
    "put_result = session.file.put(\"/home/app/CareConnect-gemma3-4b-pt/model-00009-of-00010.safetensors\",\"@SOFTWARESURGEONS_DB.GIT.CARECONNECT_GEMMA3_STAGE\", auto_compress= False, overwrite= True)\n",
    "put_result = session.file.put(\"/home/app/CareConnect-gemma3-4b-pt/model-00010-of-00010.safetensors\",\"@SOFTWARESURGEONS_DB.GIT.CARECONNECT_GEMMA3_STAGE\", auto_compress= False, overwrite= True)\n",
    "put_result = session.file.put(\"/home/app/CareConnect-gemma3-4b-pt/model.safetensors.index.json\",\"@SOFTWARESURGEONS_DB.GIT.CARECONNECT_GEMMA3_STAGE\", auto_compress= False, overwrite= True)\n",
    "put_result = session.file.put(\"/home/app/CareConnect-gemma3-4b-pt/special_tokens_map.json\",\"@SOFTWARESURGEONS_DB.GIT.CARECONNECT_GEMMA3_STAGE\", auto_compress= False, overwrite= True)\n",
    "put_result = session.file.put(\"/home/app/CareConnect-gemma3-4b-pt/tokenizer.json\",\"@SOFTWARESURGEONS_DB.GIT.CARECONNECT_GEMMA3_STAGE\", auto_compress= False, overwrite= True)\n",
    "put_result = session.file.put(\"/home/app/CareConnect-gemma3-4b-pt/tokenizer.model\",\"@SOFTWARESURGEONS_DB.GIT.CARECONNECT_GEMMA3_STAGE\", auto_compress= False, overwrite= True)\n",
    "put_result = session.file.put(\"/home/app/CareConnect-gemma3-4b-pt/tokenizer_config.json\",\"@SOFTWARESURGEONS_DB.GIT.CARECONNECT_GEMMA3_STAGE\", auto_compress= False, overwrite= True)\n",
    "put_result = session.file.put(\"/home/app/Careconnect-gemma3-huggingface/adapter_config.json\",\"@SOFTWARESURGEONS_DB.GIT.CARECONNECT_GEMMA3_STAGE\", auto_compress= False, overwrite= True)\n",
    "put_result = session.file.put(\"/home/app/Careconnect-gemma3-huggingface /adapter_model.safetensors\",\"@SOFTWARESURGEONS_DB.GIT.CARECONNECT_GEMMA3_STAGE\", auto_compress= False, overwrite= True)\n",
    "\n",
    "put_result[0].status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75e601e-b8d9-4abb-b846-4f3076b03b49",
   "metadata": {
    "language": "python",
    "name": "evaluate"
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from transformers import pipeline\n",
    "# Load the model and tokenizer into the pipeline\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Load a random sample from the test dataset\n",
    "rand_idx = randint(0, len(test_dataset))\n",
    "test_sample = test_dataset\n",
    "\n",
    "# Convert as test example into a prompt with the Gemma template\n",
    "stop_token_ids = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<end_of_turn>\")]\n",
    "prompt = pipe.tokenizer.apply_chat_template(test_sample[\"messages\"][:2], tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "# Generate our SQL query.\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=False, temperature=0.1, top_k=50, top_p=0.1, eos_token_id=stop_token_ids, disable_compile=True)\n",
    "\n",
    "# Extract the user query and original answer\n",
    "print(f\"Original Answer:\\n{test_sample['messages'][1]['content']}\")\n",
    "print(f\"Generated Answer:\\n{outputs[0]['generated_text'][len(prompt):].strip()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  },
  "lastEditStatus": {
   "authorEmail": "jwade23@pvamu.edu",
   "authorId": "8831210463455",
   "authorName": "JWADE",
   "lastEditTime": 1742967935538,
   "notebookId": "y7gissj3ojlrklfheq7i",
   "sessionId": "0e21fcb4-ec53-4414-b0f7-7243ad61c7c8"
  },
  "vscode": {
   "interpreter": {
    "hash": "55cd342d786f2f69e45d9e0e8e2f6e24538258f72ce130d7c0cbb2c4a0ff3436"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
